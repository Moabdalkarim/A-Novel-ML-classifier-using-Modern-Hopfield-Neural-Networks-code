{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6NLU1Dwqg3Dw"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import cupy as np  # CuPy library for GPU-accelerated computing with a numpy-like API\n",
        "from scipy.special import logsumexp  # Importing log-sum-exp function from SciPy for numerical stability in calculations\n",
        "from sklearn.decomposition import PCA  # Import PCA from scikit-learn for dimensionality reduction\n",
        "import time\n",
        "\n",
        "\n",
        "class Hopfield_Classifier:\n",
        "    def __init__(self,w_compress=0, PCA=0):\n",
        "        \"\"\"\n",
        "        Initialize the Hopfield Network.\n",
        "        \"\"\"\n",
        "        # Initializing variables for network memory, weights, state, and to store energies\n",
        "        self.memory = None\n",
        "        self.weights = None\n",
        "        self.pca_model = None\n",
        "        self.energies = []\n",
        "        self.w_compress= w_compress\n",
        "        self.PCA= PCA\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def shuffle_along_axis(a, axis):\n",
        "        \"\"\"\n",
        "        Shuffle the elements of an array along a specified axis.\n",
        "\n",
        "        Args:\n",
        "        a (array): The array to shuffle.\n",
        "        axis (int): The axis along which to shuffle the array.\n",
        "\n",
        "        Returns:\n",
        "        array: The shuffled array.\n",
        "        \"\"\"\n",
        "        # Generating indices to shuffle along the specified axis\n",
        "        idx = np.random.rand(*a.shape).argsort(axis=axis)\n",
        "        # Returning the shuffled array\n",
        "        return np.take_along_axis(a, idx, axis=axis)\n",
        "\n",
        "    @staticmethod\n",
        "    def calc_PCA(class_data, n_components):\n",
        "        \"\"\"\n",
        "        Calculate PCA (Principal Component Analysis) for the given data.\n",
        "\n",
        "        Args:\n",
        "        class_data (array): The data for PCA calculation.\n",
        "        n_components (int/float): Number of principal components to consider/explained variance percentage.\n",
        "\n",
        "        Returns:\n",
        "        tuple: Transformed data, PCA model, and explained variance.\n",
        "        \"\"\"\n",
        "        # Initializing PCA with specified number of components\n",
        "        pca = PCA(n_components=n_components)\n",
        "        # Transforming the data using PCA\n",
        "        transformed_data = pca.fit_transform(class_data.get())\n",
        "        # Calculating explained variance\n",
        "        explained_variance = np.sum(pca.explained_variance_ratio_)\n",
        "        # Returning transformed data, the PCA model, and explained variance\n",
        "        return transformed_data, pca, explained_variance\n",
        "\n",
        "    @staticmethod\n",
        "    def dataset_hnn_preprocess(x_train, y_train, silent=True):\n",
        "        \"\"\"\n",
        "        Preprocess the dataset for Hopfield Neural Network (HNN) training by balancing classes.\n",
        "\n",
        "        Args:\n",
        "        x_train (array): Training data features.\n",
        "        y_train (array): Training data labels.\n",
        "        PCA (int/float): Number of PCA components to apply/explained variance percentage (default 0, no PCA).\n",
        "        silent (bool): If True, suppresses print statements.\n",
        "\n",
        "        Returns:\n",
        "        tuple: List of classes data.\n",
        "        \"\"\"\n",
        "        # Finding unique targets (classes) in the training labels\n",
        "        targets = np.unique(y_train)\n",
        "        # Counting the number of samples for each class\n",
        "        classes_numbers_list = [np.argwhere(y_train == target).shape[0] for target in targets]\n",
        "        # Finding the maximum number of samples in any class\n",
        "        max_num = np.array(classes_numbers_list).max()\n",
        "\n",
        "        # List to hold processed data for each class\n",
        "        classes_list = []\n",
        "        for target in targets:\n",
        "            # Finding indices of the current target class\n",
        "            idx = np.argwhere(y_train == target)\n",
        "            # Extracting features for the current target class\n",
        "            x_trgt = x_train[idx[:, 0]]\n",
        "            # Optional print statements for data stats\n",
        "            if not silent:\n",
        "                print(f\"class {target} : {x_trgt.shape} ==> min: {x_trgt.min()}, mean: {x_trgt.mean()}, max: {x_trgt.max()}\")\n",
        "            # Balancing classes by repeating samples if necessary\n",
        "            x_trgt = np.concatenate((x_trgt, x_trgt[:max_num - len(x_trgt)]), axis=0)\n",
        "            # Appending processed data to the list\n",
        "            classes_list.append(x_trgt)\n",
        "\n",
        "        # Returning the list of processed class data\n",
        "        return classes_list\n",
        "\n",
        "    def fit(self, x_train, y_train, silent=True):\n",
        "        \"\"\"\n",
        "        Train Hopfield Neural Networks for each class in the dataset.\n",
        "\n",
        "        Args:\n",
        "        x_train (array): Training data features.\n",
        "        y_train (array): Training data labels.\n",
        "        w_compress (float): Weight compression factor (default 0, no compression).\n",
        "        PCA (int/float): Number of PCA components to apply/explained variance percentage (default 0, no PCA).\n",
        "        silent (bool): If True, suppresses print statements.\n",
        "\n",
        "        Returns:\n",
        "        object: Trained Hopfield Network model.\n",
        "        \"\"\"\n",
        "        # Recording start time for training\n",
        "        start_time = time.time()\n",
        "        # Preprocessing the dataset\n",
        "        classes_list = self.dataset_hnn_preprocess(x_train, y_train, silent)\n",
        "\n",
        "        # Initializing list to store memory for each class\n",
        "        memories_list = np.array(classes_list)\n",
        "\n",
        "        # Weight compression, if enabled\n",
        "        if self.w_compress != 0:\n",
        "            if not silent:\n",
        "                # Optional print statements for weight compression\n",
        "                print(\"Weight compression enabled by value: \", self.w_compress)\n",
        "                print(\"H_Net_trgt shape before: \", memories_list.shape)\n",
        "            # Shuffling along the second axis\n",
        "            self.shuffle_along_axis(memories_list, axis=1)\n",
        "            # Reducing the number of samples based on compression factor\n",
        "            memories_list = memories_list[:, 0:int(memories_list.shape[1] * self.w_compress), :]\n",
        "            if not silent:\n",
        "                # Optional print statements after compression\n",
        "                print(\"H_Net_trgt shape after: \", memories_list.shape)\n",
        "\n",
        "        # PCA compression, if enabled\n",
        "        if self.PCA != 0:\n",
        "            # Reshaping data for PCA\n",
        "            reshaped_memories_list = memories_list.reshape((memories_list.shape[0] * memories_list.shape[1], memories_list.shape[2]))\n",
        "            if not silent:\n",
        "                # Optional print statements for PCA\n",
        "                print(\"PCA compression enabled by value: \", self.PCA)\n",
        "                print(\"H_Net_trgt shape before: \", memories_list.shape)\n",
        "            # Applying PCA for compression\n",
        "            trans_memories_list, pca_model, explained_var = self.calc_PCA(reshaped_memories_list, self.PCA)\n",
        "            # Reshaping back after PCA\n",
        "            memories_list = trans_memories_list.reshape((memories_list.shape[0], memories_list.shape[1], -1))\n",
        "            if not silent:\n",
        "                # Optional print statements after PCA\n",
        "                print(\"Explained variance : \", explained_var)\n",
        "                print(\"H_Net_trgt shape after: \", memories_list.shape)\n",
        "\n",
        "        # Storing the processed memories in the network\n",
        "        self.memory = np.array(memories_list)\n",
        "        # Learning the weights based on the memories\n",
        "        self.network_learning()\n",
        "        # Storing the PCA model if used\n",
        "        if self.PCA != 0:\n",
        "            self.pca_model = pca_model\n",
        "        if not silent:\n",
        "            # Printing the total training time\n",
        "            print(\"\\nTraining time : \", time.time() - start_time)\n",
        "\n",
        "    def network_learning(self):\n",
        "        \"\"\"\n",
        "        Method for the network to learn or store the patterns.\n",
        "        \"\"\"\n",
        "        # Using the memories directly as weights\n",
        "        X = self.memory\n",
        "        self.weights = X\n",
        "\n",
        "    def predict(self, x_test, patch_size=-1, silent=True):\n",
        "        \"\"\"\n",
        "        Predict the class of multiple input instances using Hopfield Neural Network.\n",
        "\n",
        "        Args:\n",
        "        x_test (array): Test data features.\n",
        "        patch_size (int): Patch size for energy computation (default -1, full length).\n",
        "        PCA (bool): If True, apply PCA transformation.\n",
        "        silent (bool): If True, suppresses print statements.\n",
        "\n",
        "        Returns:\n",
        "        array: Predicted labels for the test data.\n",
        "        \"\"\"\n",
        "        # Recording the start time for prediction\n",
        "        start_time = time.time()\n",
        "        if self.pca_model != None:\n",
        "            # Start time for PCA transformation\n",
        "            pca_start_time = time.time()\n",
        "            # Applying PCA transformation if enabled\n",
        "            x_test = np.array(self.pca_model.transform(x_test.get()))\n",
        "            # End time for PCA transformation\n",
        "            pca_end_time = time.time()\n",
        "            if not silent:\n",
        "                # Printing PCA transformation time\n",
        "                print(\"PCA transform only time: \", pca_end_time - pca_start_time)\n",
        "\n",
        "        # Start time for energy computation\n",
        "        energy_start_time = time.time()\n",
        "        # Computing energy for the test data\n",
        "        self.compute_energy_ext(x_test, patch_size)\n",
        "        # Storing the predicted labels\n",
        "        y_pred = self.mem_ext\n",
        "        # End time for energy computation\n",
        "        energy_end_time = time.time()\n",
        "        if not silent:\n",
        "            # Printing inference and energy computation time\n",
        "            print(\"Inference time : \", time.time() - start_time)\n",
        "            print(\"Energy computation time: \", energy_end_time - energy_start_time)\n",
        "\n",
        "        # Returning the predicted labels\n",
        "        return(np.array(y_pred).get())\n",
        "\n",
        "    def compute_energy_ext(self, input_state, patch_size=-1):\n",
        "        \"\"\"\n",
        "        Compute the energy of the network for an external input state.\n",
        "\n",
        "        Args:\n",
        "        input_state (array): The state for which the energy is to be computed.\n",
        "        patch_size (int, optional): Size of the patch for energy computation. Defaults to -1, meaning the entire length.\n",
        "        \"\"\"\n",
        "        # Transposing the weights for computation\n",
        "        ww = np.transpose(self.weights, axes=(0, 2, 1))\n",
        "        # Defaulting patch size to the length of input if not specified\n",
        "        if patch_size == -1:\n",
        "            patch_size = len(input_state)\n",
        "\n",
        "        # List to store results for each patch\n",
        "        result_list = []\n",
        "        for i in range(0, len(input_state) // patch_size + 1):\n",
        "            # Extracting a chunk of the input state\n",
        "            A_chunk = input_state[i * patch_size:(i * patch_size + patch_size)]\n",
        "            # Computing the result for the chunk\n",
        "            result_chunk = np.dot(A_chunk, ww).get()\n",
        "            # Calculating the log sum exponent for stability\n",
        "            logsum = -(logsumexp(result_chunk.T, axis=0))\n",
        "            # Finding the argmin of the logsum\n",
        "            result_chunk_argmin = np.argmin(np.array(logsum), axis=0)\n",
        "            # Appending the result to the list\n",
        "            result_list.append(result_chunk_argmin)\n",
        "\n",
        "        # Concatenating the results to form the extended memory\n",
        "        self.mem_ext = np.concatenate(tuple(result_list), axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.datasets import mnist\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# load dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize\n",
        "x_train=(x_train.reshape((-1,28*28))/127)-1\n",
        "x_test=(x_test.reshape((-1,28*28))/127)-1\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# print sizes\n",
        "print('Train: X=%s, y=%s' % (x_train.shape, y_train.shape))\n",
        "print('Test: X=%s, y=%s' % (x_test.shape, y_test.shape))\n",
        "print(\"min:\",x_train.min(),\"mean:\",x_train.mean(),\"max:\",x_train.max())\n",
        "\n",
        "\n",
        "\n",
        "######################################################## Train the HNN models\n",
        "\n",
        "\n",
        "# train\n",
        "# Initialize the Hopfield Network\n",
        "model_HNN = Hopfield_Classifier()\n",
        "model_HNN_WC = Hopfield_Classifier(w_compress=0.5)\n",
        "model_HNN_PCA = Hopfield_Classifier(PCA=0.5,w_compress=0.5)\n",
        "\n",
        "model_HNN.fit(x_train,y_train,silent=False)\n",
        "model_HNN_WC.fit(x_train,y_train,silent=False)\n",
        "model_HNN_PCA.fit(x_train,y_train,silent=False)\n",
        "\n",
        "\n",
        "\n",
        "######################################################## Training set prediction\n",
        "\n",
        "\n",
        "# predict HNN\n",
        "print(\"\\n================ Train set HNN ================\\n\")\n",
        "y_pred = model_HNN.predict (x_train,patch_size=3000,silent=False)\n",
        "print(\"accuracy: \",accuracy_score(np.array(y_train).get(), np.array(y_pred).get()),'\\n')\n",
        "print(classification_report(np.array(y_train).get(), np.array(y_pred).get()))\n",
        "\n",
        "# predict HNN_WC\n",
        "print(\"\\n================ Train set HNN_WC ================\\n\")\n",
        "y_pred = model_HNN_WC.predict (x_train,patch_size=3000,silent=False)\n",
        "print(\"accuracy: \",accuracy_score(np.array(y_train).get(), np.array(y_pred).get()),'\\n')\n",
        "print(classification_report(np.array(y_train).get(), np.array(y_pred).get()))\n",
        "\n",
        "# predict HNN_PCA\n",
        "print(\"\\n================ Train set HNN_PCA ================\\n\")\n",
        "y_pred = model_HNN_PCA.predict (x_train,patch_size=3000,silent=False)\n",
        "print(\"accuracy: \",accuracy_score(np.array(y_train).get(), np.array(y_pred).get()),'\\n')\n",
        "print(classification_report(np.array(y_train).get(), np.array(y_pred).get()))\n",
        "\n",
        "\n",
        "\n",
        "######################################################## Test set prediction\n",
        "\n",
        "\n",
        "# predict HNN\n",
        "print(\"\\n================ Test set HNN ================\\n\")\n",
        "y_pred = model_HNN.predict (x_test,patch_size=3000,silent=False)\n",
        "print(\"accuracy: \",accuracy_score(np.array(y_test).get(), np.array(y_pred).get()),'\\n')\n",
        "print(classification_report(np.array(y_test).get(), np.array(y_pred).get()))\n",
        "\n",
        "# predict HNN_WC\n",
        "print(\"\\n================ Test set HNN_WC ================\\n\")\n",
        "y_pred = model_HNN_WC.predict (x_test,patch_size=3000,silent=False)\n",
        "print(\"accuracy: \",accuracy_score(np.array(y_test).get(), np.array(y_pred).get()),'\\n')\n",
        "print(classification_report(np.array(y_test).get(), np.array(y_pred).get()))\n",
        "\n",
        "# predict HNN_PCA\n",
        "print(\"\\n================ Test set HNN_PCA ================\\n\")\n",
        "y_pred = model_HNN_PCA.predict (x_test,patch_size=3000,silent=False)\n",
        "print(\"accuracy: \",accuracy_score(np.array(y_test).get(), np.array(y_pred).get()),'\\n')\n",
        "print(classification_report(np.array(y_test).get(), np.array(y_pred).get()))"
      ],
      "metadata": {
        "id": "omfuJRszhC_q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}